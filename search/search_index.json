{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BEAM: Token-efficient Multi-agent Inference Framework","text":"<p>BEAM is a high-performance distributed multi-agent reasoning framework that focuses on maximizing token utilization and reasoning efficiency while ensuring reasoning quality.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li> <p> Extreme Efficiency     ---     By employing an intelligent skip agent scheduling algorithm, the inference overhead in multi-turn dialogues is significantly reduced.</p> </li> <li> <p> Multi-Strategy Driven     ---     It has multiple built-in search and reasoning strategies, supporting everything from simple parallel chains to complex tree-structured search layouts.</p> </li> <li> <p> Plug-In Architecture     ---     Easily integrates with various mainstream LLM APIs (Qwen, GPT, DeepSeek) and supports custom agent logic.</p> </li> <li> <p> Deep Visualization     ---     It provides a complete visualization tool for reasoning paths, making the decision-making process of multi-agent systems clear and transparent.</p> </li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use BEAM in your research, please cite:</p> <pre><code>@software{beam2024,\n  title={BEAM: A Bayesian Energy-Aware Framework for Multi-Agent Communication Optimization under Incomplete Information},\n  author={BEAM Team},\n  year={2024},\n  url={https://github.com/erwinmsmith/BEAM}\n}\n</code></pre>"},{"location":"#navigation-instructions","title":"\ud83d\udcd6 Navigation Instructions","text":"<ul> <li>If this is your first time using it, please check out  quickstart\u3002</li> <li>For a deeper understanding of the system design, please read core architecture\u3002</li> <li>For function details, please refer to the API Reference\u3002</li> </ul>"},{"location":"#links","title":"\ud83d\udd17 Links","text":"<ul> <li> GitHub</li> <li> Problem feedback</li> </ul>"},{"location":"api-reference/apireference/","title":"API Reference","text":"<p>The BEAM API Reference provides a detailed technical overview of the framework's components. It is organized into three key areas: Core Classes for building and managing agent graphs, LLM Classes for model interfacing and registration, and Integrations for bridging BEAM with external ecosystems like LangChain and LangGraph.</p>"},{"location":"api-reference/apireference/#core-classes","title":"Core Classes","text":"<p>BEAMConfig:</p> <p>Main configuration class for BEAM systems. <pre><code>@dataclass\nclass BEAMConfig:\n    agents: List[AgentConfig]           # Agent definitions\n    num_rounds: int = 1                 # Number of reasoning rounds\n    optimization: OptimizationConfig    # Optimization settings\n    decision_method: str = \"reference\"  # \"reference\", \"majority\", \"direct\"\n    domain: str = \"\"                    # Task domain\n</code></pre></p> <p>Definitions:</p> Attribute Type Default Description <code>agents</code> <code>List[AgentConfig]</code> Required Definitions of roles and counts for agents. <code>num_rounds</code> <code>int</code> <code>1</code> Number of reasoning/interaction iterations. <code>optimization</code> <code>OptimizationConfig</code> Required Settings for pruning or efficiency strategies. <code>decision_method</code> <code>str</code> <code>\"reference\"</code> Strategy: <code>\"reference\"</code>, <code>\"majority\"</code>, or <code>\"direct\"</code>. <code>domain</code> <code>str</code> <code>\"\"</code> The specific task area (e.g., \"math\", \"medical\"). <p>AgentNode:</p> <p>Base class for all agents in the graph.</p> <pre><code>node = AgentNode(\n    id=\"unique_id\",                     # Optional, auto-generated if not provided\n    agent_name=\"Solver\",                # Agent type name\n    role=\"Problem Solver\",              # Role description\n    domain=\"math\",                      # Task domain\n    llm=llm_instance,                   # LLM for generation (optional)\n    execute_fn=custom_function,         # Custom execution (optional)\n    system_prompt=\"...\",                # System prompt for LLM\n    user_prompt_template=\"...\",         # User prompt template\n)\n\n# Execute\nresult = node.execute({\"task\": \"...\"})\nresult = await node.async_execute({\"task\": \"...\"})\n</code></pre> <p>Definitions:</p> Attribute Type Default Description <code>id</code> <code>str</code> <code>None</code> Unique identifier. Auto-generated if not provided. <code>agent_name</code> <code>str</code> Required Type name of the agent (e.g., \"Solver\"). <code>role</code> <code>str</code> Required Brief description of the agent's role/responsibility. <code>domain</code> <code>str</code> <code>\"\"</code> The task domain this node belongs to. <code>llm</code> <code>LLM</code> <code>None</code> The LLM instance used for text generation. <code>execute_fn</code> <code>Callable</code> <code>None</code> Optional custom Python function for non-LLM tasks. <code>system_prompt</code> <code>str</code> <code>\"\"</code> The system-level instruction for the LLM. <code>user_prompt_template</code> <code>str</code> <code>\"\"</code> Template for user input with <code>{variable}</code> placeholders. <p>AgentGraph:</p> <p>Manages agent connections and execution flow.</p> <pre><code>graph = AgentGraph(config)\ngraph.add_node(node)\ngraph.add_nodes([node1, node2, node3])\n\n# Run inference\nresults = await graph.run({\"task\": \"...\"}, num_rounds=2)\n</code></pre> <p>Definitions:</p> Method Arguments Returns Description <code>add_node(node)</code> <code>node: AgentNode</code> <code>None</code> Adds a single agent node to the graph. <code>add_nodes(nodes)</code> <code>nodes: List[Node]</code> <code>None</code> Batch adds multiple agent nodes. <code>run(inputs, rounds)</code> <code>dict, int</code> <code>List[Res]</code> Asynchronously executes the reasoning flow. <p>PromptSet:</p> <p>Collection of prompts for a domain.</p> <pre><code>prompts = PromptSet(name=\"domain_name\")\nprompts.add_role(role, system, user)\nprompts.set_decision_template(system, user)\nprompts.get_prompt(role, **variables)\nprompts.save(\"prompts.json\")\nprompts.load(\"prompts.json\")\n</code></pre> <p>Definitions:</p> Method Arguments Description <code>add_role(role, system, user)</code> <code>str, str, str</code> Adds a new role with specific system and user templates. <code>set_decision_template(sys, usr)</code> <code>str, str</code> Sets global templates for agent decision-making. <code>get_prompt(role, **vars)</code> <code>str, kwargs</code> Renders a prompt by injecting variables into the template. <code>save(file_path)</code> <code>str</code> Serializes the prompt set to a JSON file. <code>load(file_path)</code> <code>str</code> Loads a prompt set configuration from a JSON file. <p>PromptRegistry:</p> <p>Global registry for prompt sets. <pre><code>PromptRegistry.register(name, prompt_set)\nPromptRegistry.get(name)\nPromptRegistry.keys()\nPromptRegistry.load_from_file(name, path)\n</code></pre></p> <p>Definitions:</p> Method Arguments Returns Description <code>register(name, prompt_set)</code> <code>str, PromptSet</code> <code>None</code> Registers a <code>PromptSet</code> under a specific global name. <code>get(name)</code> <code>str</code> <code>PromptSet</code> Retrieves a registered <code>PromptSet</code> by its name. <code>keys()</code> <code>None</code> <code>List[str]</code> Returns a list of all registered <code>PromptSet</code> names. <code>load_from_file(name, path)</code> <code>str, str</code> <code>None</code> Loads a <code>PromptSet</code> from a JSON file and registers it."},{"location":"api-reference/apireference/#llm-classes","title":"LLM Classes","text":"<p>BaseLLM:</p> <p>Abstract base class for LLM implementations. <pre><code>class BaseLLM(ABC):\n    @abstractmethod\n    def gen(self, messages: List[Dict]) -&gt; str: ...\n\n    @abstractmethod\n    async def agen(self, messages: List[Dict]) -&gt; str: ...\n</code></pre></p> <p>Definitions:</p> Method Arguments Returns Description <code>gen(messages)</code> <code>List[Dict]</code> <code>str</code> Synchronous generation. Takes a list of message dictionaries (role/content). <code>agen(messages)</code> <code>List[Dict]</code> <code>str</code> Asynchronous generation. Recommended for high-concurrency multi-agent tasks. <p>LLMRegistry:</p> <p>Registry for LLM implementations. <pre><code># Get LLM instance\nllm = LLMRegistry.get(\"gpt-4o\")\nllm = LLMRegistry.get(\"deepseek-chat\")\n\n# Register custom LLM\n@LLMRegistry.register(\"custom\")\nclass CustomLLM(BaseLLM):\n    ...\n</code></pre></p> <p>Definitions:</p> Method Arguments Returns Description <code>get(name)</code> <code>str</code> <code>BaseLLM</code> Retrieves an initialized LLM instance by its registered name. <code>register(name)</code> <code>str</code> <code>Decorator</code> A decorator to register a custom class (must inherit from <code>BaseLLM</code>)."},{"location":"api-reference/apireference/#integration","title":"Integration","text":"<p>LangChain:</p> <pre><code>from beam.integrations.langchain import (\n    LangChainLLMWrapper,\n    LangChainCallbackHandler,\n    wrap_langchain_runnable,\n)\n\n# Wrap LangChain LLM for use with BEAM\nfrom langchain_openai import ChatOpenAI\nlangchain_llm = ChatOpenAI(model=\"gpt-4\")\nbeam_llm = LangChainLLMWrapper(langchain_llm)\n\n# Use with BEAM agents\nagent = create_agent_node(role=\"Solver\", llm=beam_llm, ...)\n\n# Track token usage with callback\ncallback = LangChainCallbackHandler()\n# Use callback in your LangChain chains\n\n# Wrap existing runnable\nfrom langchain_core.runnables import RunnableSequence\nwrapped = wrap_langchain_runnable(your_chain, beam_config)\n</code></pre> <p>Definitions:</p> Component Type Description <code>LangChainLLMWrapper</code> <code>Class</code> Wraps a LangChain LLM instance to make it compatible with BEAM's <code>BaseLLM</code>. <code>LangChainCallbackHandler</code> <code>Class</code> A standard callback handler to track token usage and events within LangChain. <code>wrap_langchain_runnable</code> <code>Function</code> Converts an existing LangChain <code>Runnable</code> or <code>Chain</code> into a BEAM-compatible node. <p>LangGraph:</p> <p>Registry for LLM implementations. <pre><code>from beam.integrations.langgraph import (\n    BEAMState,\n    create_beam_node,\n    create_skip_condition,\n    apply_beam_masks,\n)\n\n# Use BEAM state in your graph\nfrom langgraph.graph import StateGraph\n\nclass MyState(BEAMState):\n    custom_field: str\n\n# Create BEAM-aware node\n@create_beam_node(agent_id=\"solver\")\ndef solver_node(state: MyState) -&gt; dict:\n    # Your logic here\n    return {\"result\": \"...\"}\n\n# Create skip condition based on BEAM masks\nskip_condition = create_skip_condition(\"solver\", beam_strategy)\n\n# Build graph\ngraph = StateGraph(MyState)\ngraph.add_node(\"solver\", solver_node)\ngraph.add_conditional_edges(\"start\", skip_condition, {...})\n</code></pre></p> <p>Definitions:</p> Component Type Description <code>BEAMState</code> <code>Class</code> A specialized state class that tracks BEAM-specific metadata (masks, weights) within LangGraph. <code>create_beam_node</code> <code>Decorator</code> Transforms a standard function into a BEAM-aware node that respects pruning and optimization. <code>create_skip_condition</code> <code>Function</code> Creates routing logic for LangGraph edges based on BEAM's pruning results (skipping inactive nodes). <code>apply_beam_masks</code> <code>Function</code> Utility to filter or weight LangGraph nodes based on the current BEAM optimization state."},{"location":"examples/examples/","title":"Examples","text":"<p>This example showcases a \"Multi-Solver + Verifier\" architecture. It demonstrates how to use AgentPrune to identify which solver paths are redundant for specific math tasks, thereby reducing token consumption without sacrificing accuracy.</p>"},{"location":"examples/examples/#complete-math-solving-example","title":"Complete Math Solving Example","text":"<pre><code>import asyncio\nfrom beam import (\n    BEAMConfig, AgentConfig, OptimizationConfig,\n    AgentPrune, AgentGraph, create_agent_node,\n    PromptSet, PromptRegistry, LLMRegistry\n)\n\nasync def main():\n    # 1. Setup prompts\n    prompts = PromptSet(name=\"math\")\n    prompts.add_role(\"solver\", \n        system=\"Solve math problems step by step.\",\n        user=\"Problem: {task}\\nContext: {context}\\nSolution:\")\n    prompts.add_role(\"verifier\",\n        system=\"Verify mathematical solutions.\",\n        user=\"Problem: {task}\\nSolution: {context}\\nVerification:\")\n    PromptRegistry.register(\"math\", prompts)\n\n    # 2. Configure system\n    config = BEAMConfig(\n        agents=[\n            AgentConfig(name=\"solver\", count=3),\n            AgentConfig(name=\"verifier\", count=1),\n        ],\n        num_rounds=1,\n        optimization=OptimizationConfig(\n            strategy=\"prune\",\n            optimize_spatial=True,\n            pruning_rate=0.25,\n        )\n    )\n\n    # 3. Create agents\n    llm = LLMRegistry.get(\"gpt-4o\")\n    math_prompts = PromptRegistry.get(\"math\")\n\n    agents = []\n    for i in range(3):\n        sys, usr = math_prompts.get_prompt(\"solver\", task=\"{task}\", context=\"{context}\")\n        agents.append(create_agent_node(\n            role=f\"Solver_{i}\",\n            llm=llm,\n            system_prompt=sys,\n            user_prompt_template=usr\n        ))\n\n    sys, usr = math_prompts.get_prompt(\"verifier\", task=\"{task}\", context=\"{context}\")\n    agents.append(create_agent_node(\n        role=\"Verifier\",\n        llm=llm,\n        system_prompt=sys,\n        user_prompt_template=usr\n    ))\n\n    # 4. Build and train\n    strategy = AgentPrune(config)\n    graph = AgentGraph(config)\n    graph.add_nodes(agents)\n    strategy.set_graph(graph)\n\n    train_data = [\n        {\"task\": \"2 + 2\", \"answer\": \"4\"},\n        {\"task\": \"10 * 5\", \"answer\": \"50\"},\n        {\"task\": \"100 / 4\", \"answer\": \"25\"},\n    ]\n\n    def eval_fn(pred, ans):\n        return 1.0 if ans in pred else 0.0\n\n    stats = strategy.train(train_data, eval_fn, epochs=5)\n\n    # 5. Run inference\n    result, meta = await strategy.run({\"task\": \"What is 15% of 200?\"})\n    print(f\"Result: {result}\")\n    print(f\"Tokens saved: {meta.get('tokens_saved', 'N/A')}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Definitions:</p> Method Arguments Returns Description <code>set_graph(graph)</code> <code>AgentGraph</code> <code>None</code> Binds an <code>AgentGraph</code> to the optimizer for pruning. <code>train(data, eval_fn, epochs)</code> <code>List[dict], Callable, int</code> <code>dict</code> Trains the pruning masks using training data and an evaluation function. <code>run(inputs)</code> <code>dict</code> <code>(str, dict)</code> Executes optimized inference, returning the result and metadata (e.g., tokens saved)."},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install BEAM on your system.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>BEAM requires the following system specifications:</p> <p>Operating System:</p> <ul> <li>Linux (Ubuntu 18.04 or later recommended)</li> <li>macOS 10.14 or later</li> <li>Windows 10/11 with WSL2</li> </ul> <p>Hardware Requirements:</p> Component Recommended Python 3.9+ PyTorch 2.0.0+ OpenAI 1.0.0+"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/erwinmsmith/BEAM.git\ncd BEAM\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#step-2-create-virtual-environment","title":"Step 2: Create Virtual Environment","text":"<p>Using conda (recommended):</p> <pre><code>conda create -n beam python=3.9\nconda activate beam\n</code></pre> <p>Using venv:</p> <pre><code>python -m venv beam-env\nsource beam-env/bin/activate  # On Linux/macOS\n# beam-env\\Scripts\\activate   # On Windows\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-dependencies","title":"Step 3: Install Dependencies","text":"<pre><code># LangChain integration\npip install -e \".[langchain]\"\n\n# LangGraph integration  \npip install -e \".[langgraph]\"\n\n# Bayesian/MCMC support\npip install -e \".[bayesian]\"\n\n# All dependencies\npip install -e \".[all]\"\n\n# Development tools\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Tutorial - Train your first model in 5 minutes</li> </ul>"},{"location":"getting-started/overview/","title":"Project Overview","text":"<p>Understanding BEAM's architecture and workflow will help you use it effectively.</p>"},{"location":"getting-started/overview/#architecture-overview","title":"Architecture Overview","text":"<p>BEAM is a framework designed to optimize communication efficiency in multi-agent LLM systems while maintaining output quality under incomplete information. It addresses the challenge of high inference costs in multi-agent architectures by learning which agent connections and communications are essential using Bayesian optimization techniques. THETA consists of three main components:</p> <p>How It Works:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Multi-Agent System                             \u2502\n\u2502                                                                     \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 Agent 1 \u2502\u2500\u2500\u2500\u25b6\u2502 Agent 2 \u2502\u2500\u2500\u2500\u25b6\u2502 Agent 3 \u2502\u2500\u2500\u2500\u25b6\u2502 Decision \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502        \u2502              \u2502              \u2502                              \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                       \u2502                                             \u2502\n\u2502               BEAM Optimization                                     \u2502\n\u2502          (Bayesian Edge Learning)                                   \u2502\n\u2502                       \u25bc                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502   \u2502 Agent 1 \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Agent 3 \u2502\u2500\u2500\u2500\u25b6\u2502 Decision \u2502              \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                                                     \u2502\n\u2502   * Fewer tokens consumed    * Maintained accuracy                  \u2502\n\u2502   * Reduced latency          * Lower inference cost                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Features:</p> Feature Description Three Optimization Strategies Unsupervised learning Extensible Prompt Management Domain-specific prompt templates with registry Framework Integration Lightweight utilities for popular frameworks Flexible Agent Design Support custom functions or LLM-based agents Graph-based Architecture Spatial and temporal agent connections"},{"location":"getting-started/overview/#directory-structure","title":"Directory Structure","text":"<p>BEAM organizes files in the following structure:</p>"},{"location":"getting-started/overview/#project-directory","title":"Project Directory","text":"<pre><code>BEAM/\n\u251c\u2500\u2500 beam/\n\u2502   \u251c\u2500\u2500 __init__.py          # Package exports\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 config.py        # BEAMConfig, OptimizationConfig, AgentConfig\n\u2502   \u2502   \u251c\u2500\u2500 graph.py         # AgentGraph - manages agent connections\n\u2502   \u2502   \u251c\u2500\u2500 node.py          # AgentNode - base agent class\n\u2502   \u2502   \u251c\u2500\u2500 llm.py           # BaseLLM, LLMRegistry\n\u2502   \u2502   \u2514\u2500\u2500 optimizer.py     # TokenOptimizer - training orchestration\n\u2502   \u251c\u2500\u2500 strategies/\n\u2502   \u2502   \u251c\u2500\u2500 prune.py         # AgentPrune strategy\n\u2502   \u2502   \u251c\u2500\u2500 dropout.py       # AgentDropout strategy\n\u2502   \u2502   \u2514\u2500\u2500 bayesian.py      # AgentBayesian strategy\n\u2502   \u251c\u2500\u2500 prompts/\n\u2502   \u2502   \u251c\u2500\u2500 template.py      # PromptTemplate, PromptSet\n\u2502   \u2502   \u2514\u2500\u2500 registry.py      # PromptRegistry\n\u2502   \u251c\u2500\u2500 integrations/\n\u2502   \u2502   \u251c\u2500\u2500 langchain.py     # LangChain utilities\n\u2502   \u2502   \u2514\u2500\u2500 langgraph.py     # LangGraph utilities\n\u2502   \u2514\u2500\u2500 examples/\n\u2502       \u251c\u2500\u2500 basic_usage.py\n\u2502       \u2514\u2500\u2500 prompts_example.py\n\u251c\u2500\u2500 pyproject.toml           # Package configuration\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"getting-started/overview/#workflow-summary","title":"Workflow Summary","text":"<p>The BEAM framework follows a structured process to transform high-level objectives into optimized agent execution. The typical workflow consists of four stages:</p> <p>Stage 1: Initialization &amp; Configuration 1. Configure Settings: Define parameters in BEAMConfig, including API keys, model hyperparameters, and execution constraints.</p> <ol> <li> <p>Register LLMs: Use the LLMRegistry to register and load the underlying large language models (e.g., GPT-4, Claude, or local models).</p> </li> <li> <p>Prepare Prompts: Set up task-specific instruction templates within the PromptRegistry for dynamic retrieval.</p> </li> </ol> <p>Stage 2: Architecture Construction</p> <ol> <li>Define Nodes: Create multiple AgentNode instances, assigning specific functional roles and attributes to each.</li> <li>Build the Graph: Utilize AgentGraph to establish logical connections (Directed Acyclic Graphs) between agents.</li> <li>Initialize Optimizer: Set up the TokenOptimizer engine to oversee the orchestration and efficiency of the graph.</li> </ol> <p>Stage 3: Optimization &amp; Execution</p> <ol> <li>Run Orchestration: Execute optimizer.py to manage the task flow across the agent network.</li> <li>Apply Strategies: Implement Prune or Dropout strategies to dynamically trim redundant paths or nodes.</li> <li>Token Allocation: Real-time adjustment of token distribution based on feedback to achieve the highest performance-to-cost ratio.</li> </ol> <p>Stage 4: Output &amp; Evaluation</p> <ol> <li> <p>Generate Results: Retrieve the final output generated through the optimized execution path.</p> </li> <li> <p>Analyze Metrics: Review token consumption reports and perform a cost-benefit analysis.</p> </li> <li> <p>Validate Robustness: Use provided scripts in the examples/ directory to verify the reliability and stability of the agent collaboration.</p> </li> </ol>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture, you can:</p> <ul> <li>Three implementation strategies for BEAM strategies </li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This tutorial demonstrates how to train a BEAM model on your dataset in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Create a json file with your text data. The Json must contain a column with text content.</p> <p>Example JSOn format:</p> <pre><code>{\"task\": \"What is 15% of 80?\", \"answer\": \"12\"},\n{\"task\": \"Solve: 2x + 5 = 13\", \"answer\": \"4\"},\n{\"task\": \"Calculate 3^4\", \"answer\": \"81\"},\n</code></pre> <p>Required columns:</p> Column Name Type Required Description task / clean_text string Yes Math problem description answer float/int Yes True answer"},{"location":"getting-started/quickstart/#step-2-configure-your-system","title":"Step 2: Configure Your System","text":"<pre><code>from beam import BEAMConfig, AgentConfig, OptimizationConfig\n\nconfig = BEAMConfig(\n    # Define your agents\n    agents=[\n        AgentConfig(name=\"Analyzer\", count=2),\n        AgentConfig(name=\"Solver\", count=3),\n        AgentConfig(name=\"Verifier\", count=1),\n    ],\n\n    # Multi-round reasoning\n    num_rounds=2,\n\n    # Optimization settings\n    optimization=OptimizationConfig(\n        strategy=\"prune\",           # \"prune\", \"dropout\", or \"bayesian\"\n        optimize_spatial=True,      # Optimize same-round connections\n        optimize_temporal=True,     # Optimize cross-round connections\n        pruning_rate=0.25,          # Target 25% edge reduction\n        learning_rate=0.01,\n    ),\n\n    # Decision aggregation\n    decision_method=\"reference\",    # \"reference\", \"majority\", or \"direct\"\n    domain=\"math\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-create-agents","title":"Step 3: Create Agents","text":"<p>Option A: Using Custom Functions</p> <pre><code>from beam import create_agent_node\n\ndef analyze_problem(state):\n    \"\"\"Custom analysis logic.\"\"\"\n    task = state[\"task\"]\n    context = state.get(\"context\", \"\")\n    # Your analysis logic here\n    return f\"Analysis: The problem requires solving {task}\"\n\ndef solve_problem(state):\n    \"\"\"Custom solving logic.\"\"\"\n    task = state[\"task\"]\n    analysis = state.get(\"context\", \"\")\n    # Your solving logic here\n    return f\"Solution based on analysis: {analysis}\"\n\n# Create nodes\nanalyzer = create_agent_node(role=\"Analyzer\", execute_fn=analyze_problem)\nsolver = create_agent_node(role=\"Solver\", execute_fn=solve_problem)\n</code></pre> <p>Option B: Using LLM with Prompts</p> <pre><code>from beam import create_agent_node, LLMRegistry\n\n# Get LLM instance\nllm = LLMRegistry.get(\"gpt-4o\")\n\n# Create agent with prompts\nanalyzer = create_agent_node(\n    role=\"Analyzer\",\n    llm=llm,\n    system_prompt=\"\"\"You are a problem analyzer. Your task is to:\n1. Identify key information in the problem\n2. Determine what type of problem this is\n3. Outline the approach to solve it\"\"\",\n    user_prompt_template=\"\"\"Problem: {task}\n\nPrevious analysis (if any):\n{context}\n\nProvide your analysis:\"\"\"\n)\n\nsolver = create_agent_node(\n    role=\"Solver\", \n    llm=llm,\n    system_prompt=\"\"\"You are a problem solver. Based on the analysis provided,\nsolve the problem step by step and provide a clear final answer.\"\"\",\n    user_prompt_template=\"\"\"Problem: {task}\n\nAnalysis from other agents:\n{context}\n\nYour solution:\"\"\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-use-prompt-templates","title":"Step 4: Use Prompt Templates","text":"<pre><code>from beam import PromptSet, PromptRegistry\n\n# Create a reusable prompt set\nmath_prompts = PromptSet(name=\"math\", description=\"Math problem solving\")\n\n# Add role-specific prompts\nmath_prompts.add_role(\n    role=\"analyzer\",\n    system=\"\"\"You are a mathematical analyst. Break down problems into components:\n- Identify given information\n- Identify what needs to be found\n- Suggest solution strategies\"\"\",\n    user=\"Problem: {task}\\n\\nContext: {context}\\n\\nYour analysis:\"\n)\n\nmath_prompts.add_role(\n    role=\"solver\",\n    system=\"\"\"You are a math solver. Show your work step by step.\nBe precise with calculations and clearly state your final answer.\"\"\",\n    user=\"Problem: {task}\\n\\nAnalysis: {context}\\n\\nSolution:\"\n)\n\nmath_prompts.add_role(\n    role=\"verifier\",\n    system=\"\"\"You verify mathematical solutions. Check for:\n- Calculation errors\n- Logic errors\n- Missing steps\nConfirm or correct the answer.\"\"\",\n    user=\"Problem: {task}\\n\\nSolution to verify: {context}\\n\\nVerification:\"\n)\n\n# Set decision template\nmath_prompts.set_decision_template(\n    system=\"Synthesize multiple solutions and provide the final answer.\",\n    user=\"Problem: {task}\\n\\nSolutions:\\n{context}\\n\\nFinal answer:\"\n)\n\n# Register for global access\nPromptRegistry.register(\"math\", math_prompts)\n\n# Use anywhere in your code\nprompts = PromptRegistry.get(\"math\")\nsystem, user = prompts.get_prompt(\"solver\", task=\"2+2=?\", context=\"Simple addition\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-train-and-run-optimization","title":"Step 5: Train and Run Optimization","text":"<pre><code>from beam import AgentPrune, AgentGraph\n\n# Create strategy\nstrategy = AgentPrune(config)\n\n# Build graph (connects agents based on config)\ngraph = AgentGraph(config)\ngraph.add_nodes([analyzer, solver, verifier])\nstrategy.set_graph(graph)\n\n# Prepare training data\ntrain_data = [\n    {\"task\": \"What is 15% of 80?\", \"answer\": \"12\"},\n    {\"task\": \"Solve: 2x + 5 = 13\", \"answer\": \"4\"},\n    {\"task\": \"Calculate 3^4\", \"answer\": \"81\"},\n    # ... more examples\n]\n\n# Define evaluation function\ndef eval_fn(prediction: str, answer: str) -&gt; float:\n    \"\"\"Return 1.0 for correct, 0.0 for incorrect.\"\"\"\n    try:\n        # Extract numbers and compare\n        pred_nums = [float(s) for s in prediction.split() if s.replace('.','').isdigit()]\n        ans_num = float(answer)\n        return 1.0 if ans_num in pred_nums else 0.0\n    except:\n        return 0.0\n\n# Train (learns which edges to prune)\ntraining_stats = strategy.train(\n    train_data=train_data,\n    eval_fn=eval_fn,\n    epochs=10,\n    batch_size=4\n)\n\nprint(f\"Training accuracy: {training_stats['final_accuracy']:.2%}\")\nprint(f\"Edges pruned: {training_stats['edges_pruned']}\")\n\n# Run optimized inference\nresult, metadata = await strategy.run({\"task\": \"What is 25% of 200?\"})\nprint(f\"Answer: {result}\")\nprint(f\"Tokens used: {metadata['tokens_used']}\")\nprint(f\"Agents activated: {metadata['active_agents']}\")\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Three implementation strategies for BEAM strategies </li> <li>Examples - Real-world use cases</li> </ul>"},{"location":"models/strategies/","title":"Strategies","text":"<p>BEAM provides three specialized optimization strategies designed to balance performance and token efficiency. These strategies dynamically adjust the agent network by removing redundant paths, skipping unnecessary agents, or using probabilistic models to find the optimal configuration.</p>"},{"location":"models/strategies/#agentprune","title":"AgentPrune","text":"<p>Learns edge importance through policy gradient optimization and prunes low-importance connections.</p> <p>Training: <pre><code>from beam import AgentPrune, OptimizationConfig\n\nconfig = BEAMConfig(\n    # ...\n    optimization=OptimizationConfig(\n        strategy=\"prune\",\n        optimize_spatial=True,\n        optimize_temporal=True,\n        pruning_rate=0.3,        # Prune 30% of edges\n        initial_probability=0.5, # Initial edge probability\n    )\n)\n\nstrategy = AgentPrune(config)\n</code></pre> Best for: - Dense agent networks where many connections are redundant.</p> <p>How it works: - Initializes learnable logits for each potential edge - Uses Gumbel-Softmax for differentiable edge sampling - Optimizes via policy gradient based on task performance - Prunes edges below learned threshold</p>"},{"location":"models/strategies/#agentdropout","title":"AgentDropout","text":"<p>Learns which agents can be skipped entirely during inference. <pre><code>from beam import AgentDropout, OptimizationConfig\n\nconfig = BEAMConfig(\n    # ...\n    optimization=OptimizationConfig(\n        strategy=\"dropout\",\n        optimize_spatial=True,\n        dropout_rate=0.2,        # Target 20% agent skip rate\n    )\n)\n\nstrategy = AgentDropout(config)\n</code></pre> Best for: - Systems with redundant agents where some can be skipped without quality loss.</p> <p>How it works:: - Learns skip probability for each agent - During training, samples skip decisions - Agents with high skip probability are dropped during inference - Maintains ensemble diversity while reducing computation</p>"},{"location":"models/strategies/#agentbayesian","title":"AgentBayesian","text":"<p>Uses Bayesian optimization with optional MCMC sampling for uncertainty-aware pruning.</p> <pre><code>from beam import AgentBayesian, OptimizationConfig\n\nconfig = BEAMConfig(\n    # ...\n    optimization=OptimizationConfig(\n        strategy=\"bayesian\",\n        optimize_spatial=True,\n        optimize_temporal=True,\n        use_mcmc=True,           # Enable MCMC sampling\n        mcmc_samples=100,        # Number of MCMC samples\n        prior_mean=0.5,          # Prior edge probability\n    )\n)\n\nstrategy = AgentBayesian(config)\n</code></pre> <p>Best for: - When you need confidence estimates or have limited training data.</p> <p>How it works: - Maintains posterior distribution over edge weights - Uses MCMC (optional) to sample from posterior - Provides uncertainty estimates for pruning decisions - More robust with small datasets</p>"}]}